{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emisoft-designs/SEEFAR-SUSTAIN/blob/academy-data-extraction-v3/SEEFAR_SUSTAIN_(Academy_DB_Extraction).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-IHWbfzQ90C"
      },
      "source": [
        "# **Colab Notebook – Data Extraction from SEEFAR Academy Database**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRjSE8sRISxi",
        "outputId": "0d87b9fb-2613-4eaa-8d9a-64272ec666c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymysql in /usr/local/lib/python3.12/dist-packages (1.1.2)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.12/dist-packages (2.0.46)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.12/dist-packages (6.2.1)\n",
            "Requirement already satisfied: gspread-dataframe in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy) (3.3.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy) (4.15.0)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from gspread) (2.47.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from gspread) (1.2.4)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from gspread-dataframe) (2.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from gspread-dataframe) (1.17.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.12.0->gspread) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.12.0->gspread) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.0->gspread-dataframe) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.0->gspread-dataframe) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.0->gspread-dataframe) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.0->gspread-dataframe) (2025.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread) (0.6.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.3.1)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2026.1.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install pymysql sqlalchemy gspread gspread-dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4zI_7m9DsiPw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f1fcb60-8ccb-4d37-9b6c-ac10becc4275"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GAyfuoc9OvHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83863fda-27f8-4f2d-8be0-f225b47d3dc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Retrieved database password from Colab secrets\n",
            "✓ Database engine created successfully\n",
            "\n",
            "================================================================================\n",
            "STARTING COMPREHENSIVE DATA EXTRACTION\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "EXTRACTING USER DATA\n",
            "================================================================================\n",
            "✓ Fetched 2014 rows × 22 columns\n",
            "✓ Fetched 5019 rows × 3 columns\n",
            "✓ Extracted 2014 users with demographics\n",
            "\n",
            "================================================================================\n",
            "EXTRACTING COURSE DATA\n",
            "================================================================================\n",
            "✓ Fetched 30 rows × 14 columns\n",
            "✓ Extracted 30 courses\n",
            "\n",
            "Course distribution by training track:\n",
            "training_track\n",
            "Other                           29\n",
            "General Preparatory Training     1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "================================================================================\n",
            "EXTRACTING USER ROLES\n",
            "================================================================================\n",
            "✓ Fetched 1506 rows × 9 columns\n",
            "✓ Extracted 1506 role assignments\n",
            "\n",
            "User type distribution:\n",
            "user_type_category\n",
            "Participant/Candidate     1471\n",
            "Other                       16\n",
            "Manager                     12\n",
            "Facilitator/Instructor       7\n",
            "Name: count, dtype: int64\n",
            "\n",
            "================================================================================\n",
            "EXTRACTING ENROLLMENT DATA\n",
            "================================================================================\n",
            "✓ Fetched 1483 rows × 13 columns\n",
            "✓ Extracted 1483 enrollments\n",
            "\n",
            "================================================================================\n",
            "EXTRACTING QUIZ/ASSESSMENT DATA\n",
            "================================================================================\n",
            "✓ Fetched 301 rows × 11 columns\n",
            "✓ Fetched 6960 rows × 12 columns\n",
            "✓ Fetched 5354 rows × 6 columns\n",
            "✓ Extracted 301 quizzes\n",
            "✓ Extracted 6960 quiz attempts\n",
            "✓ Extracted 5354 quiz grades\n",
            "\n",
            "Quiz type distribution:\n",
            "assessment_type\n",
            "Assessment    297\n",
            "Pre-Test        2\n",
            "Post-Test       2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "================================================================================\n",
            "EXTRACTING ASSIGNMENT DATA\n",
            "================================================================================\n",
            "✓ Fetched 4 rows × 9 columns\n",
            "✓ Fetched 55 rows × 10 columns\n",
            "✓ Fetched 0 rows × 10 columns\n",
            "✓ Extracted 4 assignments\n",
            "✓ Extracted 55 submissions\n",
            "✓ Extracted 0 assignment grades\n",
            "\n",
            "================================================================================\n",
            "EXTRACTING FEEDBACK/SURVEY DATA\n",
            "================================================================================\n",
            "✓ Fetched 64 rows × 8 columns\n",
            "✓ Fetched 1378 rows × 5 columns\n",
            "✓ Fetched 3064 rows × 6 columns\n",
            "✓ Extracted 64 feedback activities\n",
            "✓ Extracted 1378 feedback responses\n",
            "✓ Extracted 3064 questionnaire responses\n",
            "\n",
            "================================================================================\n",
            "EXTRACTING COMPLETION DATA\n",
            "================================================================================\n",
            "✓ Fetched 372 rows × 10 columns\n",
            "✓ Fetched 20102 rows × 6 columns\n",
            "✓ Fetched 564 rows × 6 columns\n",
            "✓ Extracted 372 course completions\n",
            "✓ Extracted 20102 module completions\n",
            "✓ Extracted 564 certificates issued\n",
            "\n",
            "================================================================================\n",
            "EXTRACTING ACTIVITY LOGS\n",
            "================================================================================\n",
            "✓ Fetched 100000 rows × 11 columns\n",
            "✓ Extracted 100000 activity log entries\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_auth_httplib2:httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n",
            "WARNING:google_auth_httplib2:httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "DATA EXTRACTION COMPLETE\n",
            "================================================================================\n",
            "\n",
            "Total datasets extracted: 17\n",
            "\n",
            "Dataset summary:\n",
            "  - users: 2014 records\n",
            "  - courses: 30 records\n",
            "  - user_roles: 1506 records\n",
            "  - enrollments: 1483 records\n",
            "  - quizzes: 301 records\n",
            "  - quiz_attempts: 6960 records\n",
            "  - quiz_grades: 5354 records\n",
            "  - assignments: 4 records\n",
            "  - assignment_submissions: 55 records\n",
            "  - assignment_grades: 0 records\n",
            "  - feedback: 64 records\n",
            "  - feedback_responses: 1378 records\n",
            "  - questionnaire_responses: 3064 records\n",
            "  - course_completions: 372 records\n",
            "  - module_completions: 20102 records\n",
            "  - certificates: 564 records\n",
            "  - activity_logs: 100000 records\n",
            "Opening existing: https://docs.google.com/spreadsheets/d/1rF_HoRlOUPjBEnmyIo7wy-MGgQ0CB0Pbli3j9IU2PaQ\n",
            "Exported users\n",
            "Exported courses\n",
            "Exported user_roles\n",
            "Exported enrollments\n",
            "Exported quizzes\n",
            "Exported quiz_attempts\n",
            "Exported quiz_grades\n",
            "Exported assignments\n",
            "Exported assignment_submissions\n",
            "Exported feedback\n",
            "Exported feedback_responses\n",
            "Exported questionnaire_responses\n",
            "Exported course_completions\n",
            "Exported module_completions\n",
            "Exported certificates\n",
            "Exported activity_logs\n",
            "\n",
            "================================================================================\n",
            "EXECUTION COMPLETE\n",
            "================================================================================\n",
            "\n",
            "Summary:\n",
            "  - Total datasets extracted: 17 \n",
            "  - Documentation generated: MEL_Data_Documentation.md\n",
            "  - Google Sheets URL: https://docs.google.com/spreadsheets/d/1rF_HoRlOUPjBEnmyIo7wy-MGgQ0CB0Pbli3j9IU2PaQ\n",
            "\n",
            "Next Steps:\n",
            "  1. Review the Google Sheets export\n",
            "  2. Read the documentation file\n",
            "  3. Set up automated dashboards\n",
            "  4. Configure scheduled reporting\n",
            "\n",
            "For support, refer to the documentation or contact the data team.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "SUSTAIN MEL Data Extraction & Documentation Script\n",
        "====================================================\n",
        "\n",
        "Purpose: Extract, structure, and document Moodle database data for Monitoring,\n",
        "         Evaluation, and Learning (MEL) indicators for SUSTAIN training programs.\n",
        "\n",
        "Author: Seefar Academy Data Team\n",
        "Date: 2026-02-03\n",
        "Version: 2.0\n",
        "\n",
        "Database: Seefar Academy Pathways Moodle (pathways_moodle)\n",
        "Target: MEL reporting for SUSTAIN, Onboarding, and General Preparatory Training\n",
        "\n",
        "Key Objectives:\n",
        "1. Map database schema for training delivery, assessments, users, and roles\n",
        "2. Link Course ID ↔ Course Name for training track filtering\n",
        "3. Standardize login dates (First Login, Last Login)\n",
        "4. Extract and relate pre-test/post-test results to users and courses\n",
        "5. Extract demographics for disaggregation (name, email, phone, gender, country)\n",
        "6. Differentiate user types (Participants, Candidates, Employers, Visa Facilitators)\n",
        "7. Support MEL indicator calculations (completion rates, knowledge increase, etc.)\n",
        "8. Enable automated reporting and dashboarding\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 1: SETUP & CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# Install required packages\n",
        "# !pip install pymysql sqlalchemy gspread gspread-dataframe pandas numpy\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "from sqlalchemy import create_engine, text\n",
        "from typing import Dict, List, Tuple, Any, Optional\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import json\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Database credentials configuration\n",
        "class DatabaseConfig:\n",
        "    \"\"\"Database connection configuration\"\"\"\n",
        "    DB_USER = \"dbuserread\"\n",
        "    # DB_PASSWORD should be set via environment variable or Google Colab userdata\n",
        "    DB_HOST = \"ls-17122ea7e3a528fd292a260b6217b006cb7a0f38.cyhh372xsm2h.ap-southeast-1.rds.amazonaws.com\"\n",
        "    DB_PORT = 3306\n",
        "    DB_NAME = \"pathways_moodle\"\n",
        "\n",
        "    @classmethod\n",
        "    def get_connection_url(cls, password: str) -> str:\n",
        "        \"\"\"Generate database connection URL\"\"\"\n",
        "        return f\"mysql+pymysql://{cls.DB_USER}:{password}@{cls.DB_HOST}:{cls.DB_PORT}/{cls.DB_NAME}\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 2: DATABASE UTILITY FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def create_db_engine(password: str):\n",
        "    \"\"\"\n",
        "    Create SQLAlchemy engine for database connection.\n",
        "\n",
        "    Args:\n",
        "        password: Database password\n",
        "\n",
        "    Returns:\n",
        "        SQLAlchemy engine object\n",
        "    \"\"\"\n",
        "    connection_url = DatabaseConfig.get_connection_url(password)\n",
        "    engine = create_engine(connection_url, pool_pre_ping=True)\n",
        "    print(\"✓ Database engine created successfully\")\n",
        "    return engine\n",
        "\n",
        "\n",
        "def fetch_data(engine, query: str, params: Optional[Dict] = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Execute SQL query and return results as DataFrame.\n",
        "\n",
        "    Args:\n",
        "        engine: SQLAlchemy engine\n",
        "        query: SQL query string\n",
        "        params: Optional query parameters\n",
        "\n",
        "    Returns:\n",
        "        pandas DataFrame with query results\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with engine.connect() as conn:\n",
        "            if params:\n",
        "                df = pd.read_sql(text(query), conn, params=params)\n",
        "            else:\n",
        "                df = pd.read_sql(query, conn)\n",
        "        print(f\"✓ Fetched {df.shape[0]} rows × {df.shape[1]} columns\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error fetching data: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "def get_table_info(engine, table_name: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Get column information for a specific table.\n",
        "\n",
        "    Args:\n",
        "        engine: SQLAlchemy engine\n",
        "        table_name: Name of the table\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with column information\n",
        "    \"\"\"\n",
        "    query = f\"\"\"\n",
        "    SELECT\n",
        "        COLUMN_NAME as column_name,\n",
        "        DATA_TYPE as data_type,\n",
        "        IS_NULLABLE as is_nullable,\n",
        "        COLUMN_KEY as column_key,\n",
        "        COLUMN_COMMENT as column_comment\n",
        "    FROM INFORMATION_SCHEMA.COLUMNS\n",
        "    WHERE TABLE_SCHEMA = '{DatabaseConfig.DB_NAME}'\n",
        "    AND TABLE_NAME = '{table_name}'\n",
        "    ORDER BY ORDINAL_POSITION\n",
        "    \"\"\"\n",
        "    return fetch_data(engine, query)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 3: CORE DATA EXTRACTION FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "class MELDataExtractor:\n",
        "    \"\"\"Main class for extracting MEL-relevant data from Moodle database\"\"\"\n",
        "\n",
        "    def __init__(self, engine):\n",
        "        self.engine = engine\n",
        "        self.extracted_data = {}\n",
        "\n",
        "    def extract_users(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Extract user data with demographic information.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with user information including demographics\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"EXTRACTING USER DATA\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        query = \"\"\"\n",
        "        SELECT\n",
        "            u.id as user_id,\n",
        "            u.username,\n",
        "            u.firstname,\n",
        "            u.lastname,\n",
        "            u.email,\n",
        "            u.phone1 as phone,\n",
        "            u.country,\n",
        "            u.city,\n",
        "            u.institution,\n",
        "            u.department,\n",
        "            u.timecreated as user_created_timestamp,\n",
        "            u.timemodified as user_modified_timestamp,\n",
        "            u.firstaccess as first_login_timestamp,\n",
        "            u.lastaccess as last_login_timestamp,\n",
        "            u.lastlogin as last_login_timestamp_alt,\n",
        "            u.currentlogin as current_login_timestamp,\n",
        "            u.confirmed,\n",
        "            u.suspended,\n",
        "            u.deleted,\n",
        "            FROM_UNIXTIME(u.timecreated) as user_created_date,\n",
        "            FROM_UNIXTIME(u.firstaccess) as first_login_date,\n",
        "            FROM_UNIXTIME(u.lastaccess) as last_login_date\n",
        "        FROM mdl_user u\n",
        "        WHERE u.deleted = 0\n",
        "        AND u.id > 2  -- Exclude guest and admin system accounts\n",
        "        ORDER BY u.id\n",
        "        \"\"\"\n",
        "\n",
        "        users_df = fetch_data(self.engine, query)\n",
        "\n",
        "        # Add custom fields (demographics)\n",
        "        demographics_query = \"\"\"\n",
        "        SELECT\n",
        "            uid.userid as user_id,\n",
        "            uif.shortname as field_name,\n",
        "            uid.data as field_value\n",
        "        FROM mdl_user_info_data uid\n",
        "        JOIN mdl_user_info_field uif ON uid.fieldid = uif.id\n",
        "        WHERE uif.shortname IN ('gender', 'age', 'agegroup', 'nationality', 'ethnicity')\n",
        "        \"\"\"\n",
        "\n",
        "        demographics_df = fetch_data(self.engine, demographics_query)\n",
        "\n",
        "        # Pivot demographics data\n",
        "        if not demographics_df.empty:\n",
        "            demographics_pivot = demographics_df.pivot(\n",
        "                index='user_id',\n",
        "                columns='field_name',\n",
        "                values='field_value'\n",
        "            ).reset_index()\n",
        "\n",
        "            users_df = users_df.merge(demographics_pivot, on='user_id', how='left')\n",
        "\n",
        "        print(f\"✓ Extracted {len(users_df)} users with demographics\")\n",
        "        self.extracted_data['users'] = users_df\n",
        "        return users_df\n",
        "\n",
        "    def extract_courses(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Extract course information with categorization.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with course details\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"EXTRACTING COURSE DATA\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        query = \"\"\"\n",
        "        SELECT\n",
        "            c.id as course_id,\n",
        "            c.category as course_category_id,\n",
        "            c.fullname as course_name,\n",
        "            c.shortname as course_shortname,\n",
        "            c.summary as course_description,\n",
        "            c.startdate as course_start_timestamp,\n",
        "            c.enddate as course_end_timestamp,\n",
        "            FROM_UNIXTIME(c.startdate) as course_start_date,\n",
        "            FROM_UNIXTIME(c.enddate) as course_end_date,\n",
        "            c.visible as is_visible,\n",
        "            c.timecreated as course_created_timestamp,\n",
        "            FROM_UNIXTIME(c.timecreated) as course_created_date,\n",
        "            cc.name as category_name,\n",
        "            cc.path as category_path\n",
        "        FROM mdl_course c\n",
        "        LEFT JOIN mdl_course_categories cc ON c.category = cc.id\n",
        "        WHERE c.id > 1  -- Exclude site home course\n",
        "        ORDER BY c.id\n",
        "        \"\"\"\n",
        "\n",
        "        courses_df = fetch_data(self.engine, query)\n",
        "\n",
        "        # Categorize courses based on name patterns\n",
        "        def categorize_course(row):\n",
        "            \"\"\"Categorize course into training tracks\"\"\"\n",
        "            course_name = str(row['course_name']).lower()\n",
        "            course_short = str(row['course_shortname']).lower()\n",
        "\n",
        "            if 'sustain' in course_name or 'sustain' in course_short:\n",
        "                return 'SUSTAIN Training'\n",
        "            elif 'onboard' in course_name or 'onboard' in course_short:\n",
        "                return 'Onboarding Training'\n",
        "            elif 'prep' in course_name or 'career' in course_name or 'stem' in course_name:\n",
        "                return 'General Preparatory Training'\n",
        "            elif 'employer' in course_name:\n",
        "                return 'Employer Training'\n",
        "            elif 'visa' in course_name or 'facilitator' in course_name:\n",
        "                return 'Visa Facilitator Training'\n",
        "            else:\n",
        "                return 'Other'\n",
        "\n",
        "        courses_df['training_track'] = courses_df.apply(categorize_course, axis=1)\n",
        "\n",
        "        print(f\"✓ Extracted {len(courses_df)} courses\")\n",
        "        print(\"\\nCourse distribution by training track:\")\n",
        "        print(courses_df['training_track'].value_counts())\n",
        "\n",
        "        self.extracted_data['courses'] = courses_df\n",
        "        return courses_df\n",
        "\n",
        "    def extract_user_roles(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Extract user role assignments to differentiate user types.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with user roles\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"EXTRACTING USER ROLES\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        query = \"\"\"\n",
        "        SELECT\n",
        "            ra.id as role_assignment_id,\n",
        "            ra.userid as user_id,\n",
        "            ra.roleid as role_id,\n",
        "            r.shortname as role_shortname,\n",
        "            r.name as role_name,\n",
        "            ra.contextid,\n",
        "            c.contextlevel,\n",
        "            c.instanceid,\n",
        "            FROM_UNIXTIME(ra.timemodified) as role_assigned_date\n",
        "        FROM mdl_role_assignments ra\n",
        "        JOIN mdl_role r ON ra.roleid = r.id\n",
        "        JOIN mdl_context c ON ra.contextid = c.id\n",
        "        ORDER BY ra.userid, ra.roleid\n",
        "        \"\"\"\n",
        "\n",
        "        roles_df = fetch_data(self.engine, query)\n",
        "\n",
        "        # Categorize user types based on roles\n",
        "        def categorize_user_type(role_shortname):\n",
        "            \"\"\"Map role to user type category\"\"\"\n",
        "            role_lower = str(role_shortname).lower()\n",
        "\n",
        "            if 'student' in role_lower:\n",
        "                return 'Participant/Candidate'\n",
        "            elif 'teacher' in role_lower or 'editingteacher' in role_lower:\n",
        "                return 'Facilitator/Instructor'\n",
        "            elif 'employer' in role_lower:\n",
        "                return 'Employer'\n",
        "            elif 'manager' in role_lower:\n",
        "                return 'Manager'\n",
        "            else:\n",
        "                return 'Other'\n",
        "\n",
        "        roles_df['user_type_category'] = roles_df['role_shortname'].apply(categorize_user_type)\n",
        "\n",
        "        print(f\"✓ Extracted {len(roles_df)} role assignments\")\n",
        "        print(\"\\nUser type distribution:\")\n",
        "        print(roles_df['user_type_category'].value_counts())\n",
        "\n",
        "        self.extracted_data['user_roles'] = roles_df\n",
        "        return roles_df\n",
        "\n",
        "    def extract_enrollments(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Extract user course enrollments.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with enrollment data\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"EXTRACTING ENROLLMENT DATA\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        query = \"\"\"\n",
        "        SELECT\n",
        "            ue.id as enrollment_id,\n",
        "            ue.userid as user_id,\n",
        "            ue.enrolid,\n",
        "            e.courseid as course_id,\n",
        "            e.enrol as enrollment_method,\n",
        "            ue.status as enrollment_status,\n",
        "            ue.timestart as enrollment_start_timestamp,\n",
        "            ue.timeend as enrollment_end_timestamp,\n",
        "            ue.timecreated as enrollment_created_timestamp,\n",
        "            ue.timemodified as enrollment_modified_timestamp,\n",
        "            FROM_UNIXTIME(ue.timecreated) as enrollment_date,\n",
        "            FROM_UNIXTIME(ue.timestart) as enrollment_start_date,\n",
        "            FROM_UNIXTIME(ue.timeend) as enrollment_end_date\n",
        "        FROM mdl_user_enrolments ue\n",
        "        JOIN mdl_enrol e ON ue.enrolid = e.id\n",
        "        ORDER BY ue.userid, e.courseid\n",
        "        \"\"\"\n",
        "\n",
        "        enrollments_df = fetch_data(self.engine, query)\n",
        "\n",
        "        # Add enrollment status label\n",
        "        enrollments_df['enrollment_status_label'] = enrollments_df['enrollment_status'].map({\n",
        "            0: 'Active',\n",
        "            1: 'Suspended'\n",
        "        })\n",
        "\n",
        "        print(f\"✓ Extracted {len(enrollments_df)} enrollments\")\n",
        "\n",
        "        self.extracted_data['enrollments'] = enrollments_df\n",
        "        return enrollments_df\n",
        "\n",
        "    def extract_quiz_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Extract quiz, quiz attempts, and quiz grades for pre/post-test analysis.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (quizzes, attempts, grades) DataFrames\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"EXTRACTING QUIZ/ASSESSMENT DATA\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Quizzes\n",
        "        quiz_query = \"\"\"\n",
        "        SELECT\n",
        "            q.id as quiz_id,\n",
        "            q.course as course_id,\n",
        "            q.name as quiz_name,\n",
        "            q.intro as quiz_description,\n",
        "            q.timeopen as quiz_open_timestamp,\n",
        "            q.timeclose as quiz_close_timestamp,\n",
        "            q.timelimit as time_limit_seconds,\n",
        "            q.grade as max_grade,\n",
        "            q.sumgrades as sum_grades,\n",
        "            FROM_UNIXTIME(q.timeopen) as quiz_open_date,\n",
        "            FROM_UNIXTIME(q.timeclose) as quiz_close_date\n",
        "        FROM mdl_quiz q\n",
        "        ORDER BY q.course, q.id\n",
        "        \"\"\"\n",
        "        quizzes_df = fetch_data(self.engine, quiz_query)\n",
        "\n",
        "        # Categorize quizzes as pre-test or post-test\n",
        "        def categorize_quiz(quiz_name):\n",
        "            \"\"\"Identify pre-test vs post-test\"\"\"\n",
        "            name_lower = str(quiz_name).lower()\n",
        "            if 'pre' in name_lower or 'pre-test' in name_lower or 'pretest' in name_lower:\n",
        "                return 'Pre-Test'\n",
        "            elif 'post' in name_lower or 'post-test' in name_lower or 'posttest' in name_lower:\n",
        "                return 'Post-Test'\n",
        "            else:\n",
        "                return 'Assessment'\n",
        "\n",
        "        quizzes_df['assessment_type'] = quizzes_df['quiz_name'].apply(categorize_quiz)\n",
        "\n",
        "        # Quiz attempts\n",
        "        attempts_query = \"\"\"\n",
        "        SELECT\n",
        "            qa.id as attempt_id,\n",
        "            qa.quiz as quiz_id,\n",
        "            qa.userid as user_id,\n",
        "            qa.attempt as attempt_number,\n",
        "            qa.state as attempt_state,\n",
        "            qa.timestart as attempt_start_timestamp,\n",
        "            qa.timefinish as attempt_finish_timestamp,\n",
        "            qa.timemodified as attempt_modified_timestamp,\n",
        "            qa.sumgrades as attempt_score,\n",
        "            FROM_UNIXTIME(qa.timestart) as attempt_start_date,\n",
        "            FROM_UNIXTIME(qa.timefinish) as attempt_finish_date,\n",
        "            TIMESTAMPDIFF(SECOND, FROM_UNIXTIME(qa.timestart), FROM_UNIXTIME(qa.timefinish)) as duration_seconds\n",
        "        FROM mdl_quiz_attempts qa\n",
        "        WHERE qa.state = 'finished'\n",
        "        ORDER BY qa.userid, qa.quiz, qa.attempt\n",
        "        \"\"\"\n",
        "        attempts_df = fetch_data(self.engine, attempts_query)\n",
        "\n",
        "        # Quiz grades\n",
        "        grades_query = \"\"\"\n",
        "        SELECT\n",
        "            qg.id as grade_id,\n",
        "            qg.quiz as quiz_id,\n",
        "            qg.userid as user_id,\n",
        "            qg.grade as final_grade,\n",
        "            qg.timemodified as grade_timestamp,\n",
        "            FROM_UNIXTIME(qg.timemodified) as grade_date\n",
        "        FROM mdl_quiz_grades qg\n",
        "        ORDER BY qg.userid, qg.quiz\n",
        "        \"\"\"\n",
        "        grades_df = fetch_data(self.engine, grades_query)\n",
        "\n",
        "        print(f\"✓ Extracted {len(quizzes_df)} quizzes\")\n",
        "        print(f\"✓ Extracted {len(attempts_df)} quiz attempts\")\n",
        "        print(f\"✓ Extracted {len(grades_df)} quiz grades\")\n",
        "        print(\"\\nQuiz type distribution:\")\n",
        "        print(quizzes_df['assessment_type'].value_counts())\n",
        "\n",
        "        self.extracted_data['quizzes'] = quizzes_df\n",
        "        self.extracted_data['quiz_attempts'] = attempts_df\n",
        "        self.extracted_data['quiz_grades'] = grades_df\n",
        "\n",
        "        return quizzes_df, attempts_df, grades_df\n",
        "\n",
        "    def extract_assignment_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Extract assignment, submission, and grading data.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (assignments, submissions, grades) DataFrames\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"EXTRACTING ASSIGNMENT DATA\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Assignments\n",
        "        assign_query = \"\"\"\n",
        "        SELECT\n",
        "            a.id as assignment_id,\n",
        "            a.course as course_id,\n",
        "            a.name as assignment_name,\n",
        "            a.intro as assignment_description,\n",
        "            a.duedate as due_timestamp,\n",
        "            a.allowsubmissionsfromdate as submissions_from_timestamp,\n",
        "            a.grade as max_grade,\n",
        "            FROM_UNIXTIME(a.duedate) as due_date,\n",
        "            FROM_UNIXTIME(a.allowsubmissionsfromdate) as submissions_from_date\n",
        "        FROM mdl_assign a\n",
        "        ORDER BY a.course, a.id\n",
        "        \"\"\"\n",
        "        assignments_df = fetch_data(self.engine, assign_query)\n",
        "\n",
        "        # Submissions\n",
        "        submissions_query = \"\"\"\n",
        "        SELECT\n",
        "            asub.id as submission_id,\n",
        "            asub.assignment as assignment_id,\n",
        "            asub.userid as user_id,\n",
        "            asub.status as submission_status,\n",
        "            asub.timecreated as submission_created_timestamp,\n",
        "            asub.timemodified as submission_modified_timestamp,\n",
        "            asub.attemptnumber as attempt_number,\n",
        "            asub.latest as is_latest,\n",
        "            FROM_UNIXTIME(asub.timecreated) as submission_created_date,\n",
        "            FROM_UNIXTIME(asub.timemodified) as submission_modified_date\n",
        "        FROM mdl_assign_submission asub\n",
        "        ORDER BY asub.userid, asub.assignment\n",
        "        \"\"\"\n",
        "        submissions_df = fetch_data(self.engine, submissions_query)\n",
        "\n",
        "        # Grades\n",
        "        assign_grades_query = \"\"\"\n",
        "        SELECT\n",
        "            ag.id as grade_id,\n",
        "            ag.assignment as assignment_id,\n",
        "            ag.userid as user_id,\n",
        "            ag.grade as grade,\n",
        "            ag.grader as grader_id,\n",
        "            ag.attemptnumber as attempt_number,\n",
        "            ag.timecreated as grade_created_timestamp,\n",
        "            ag.timemodified as grade_modified_timestamp,\n",
        "            FROM_UNIXTIME(ag.timecreated) as grade_created_date,\n",
        "            FROM_UNIXTIME(ag.timemodified) as grade_modified_date\n",
        "        FROM mdl_assign_grades ag\n",
        "        WHERE ag.grade >= 0  -- Exclude ungraded submissions\n",
        "        ORDER BY ag.userid, ag.assignment\n",
        "        \"\"\"\n",
        "        assign_grades_df = fetch_data(self.engine, assign_grades_query)\n",
        "\n",
        "        print(f\"✓ Extracted {len(assignments_df)} assignments\")\n",
        "        print(f\"✓ Extracted {len(submissions_df)} submissions\")\n",
        "        print(f\"✓ Extracted {len(assign_grades_df)} assignment grades\")\n",
        "\n",
        "        self.extracted_data['assignments'] = assignments_df\n",
        "        self.extracted_data['assignment_submissions'] = submissions_df\n",
        "        self.extracted_data['assignment_grades'] = assign_grades_df\n",
        "\n",
        "        return assignments_df, submissions_df, assign_grades_df\n",
        "\n",
        "    def extract_feedback_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Extract feedback/survey responses for training usefulness assessment.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (feedback_activities, responses) DataFrames\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"EXTRACTING FEEDBACK/SURVEY DATA\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Feedback activities\n",
        "        feedback_query = \"\"\"\n",
        "        SELECT\n",
        "            f.id as feedback_id,\n",
        "            f.course as course_id,\n",
        "            f.name as feedback_name,\n",
        "            f.intro as feedback_description,\n",
        "            f.timeopen as feedback_open_timestamp,\n",
        "            f.timeclose as feedback_close_timestamp,\n",
        "            FROM_UNIXTIME(f.timeopen) as feedback_open_date,\n",
        "            FROM_UNIXTIME(f.timeclose) as feedback_close_date\n",
        "        FROM mdl_feedback f\n",
        "        ORDER BY f.course, f.id\n",
        "        \"\"\"\n",
        "        feedback_df = fetch_data(self.engine, feedback_query)\n",
        "\n",
        "        # Completed responses\n",
        "        responses_query = \"\"\"\n",
        "        SELECT\n",
        "            fc.id as response_id,\n",
        "            fc.feedback as feedback_id,\n",
        "            fc.userid as user_id,\n",
        "            fc.timemodified as response_timestamp,\n",
        "            FROM_UNIXTIME(fc.timemodified) as response_date\n",
        "        FROM mdl_feedback_completed fc\n",
        "        ORDER BY fc.userid, fc.feedback\n",
        "        \"\"\"\n",
        "        responses_df = fetch_data(self.engine, responses_query)\n",
        "\n",
        "        # Questionnaire data\n",
        "        questionnaire_query = \"\"\"\n",
        "        SELECT\n",
        "            qr.id as questionnaire_response_id,\n",
        "            qr.questionnaireid as questionnaire_id,\n",
        "            qr.userid as user_id,\n",
        "            qr.submitted as submission_timestamp,\n",
        "            qr.complete as is_complete,\n",
        "            FROM_UNIXTIME(qr.submitted) as submission_date\n",
        "        FROM mdl_questionnaire_response qr\n",
        "        ORDER BY qr.userid, qr.questionnaireid\n",
        "        \"\"\"\n",
        "        questionnaire_df = fetch_data(self.engine, questionnaire_query)\n",
        "\n",
        "        print(f\"✓ Extracted {len(feedback_df)} feedback activities\")\n",
        "        print(f\"✓ Extracted {len(responses_df)} feedback responses\")\n",
        "        print(f\"✓ Extracted {len(questionnaire_df)} questionnaire responses\")\n",
        "\n",
        "        self.extracted_data['feedback'] = feedback_df\n",
        "        self.extracted_data['feedback_responses'] = responses_df\n",
        "        self.extracted_data['questionnaire_responses'] = questionnaire_df\n",
        "\n",
        "        return feedback_df, responses_df\n",
        "\n",
        "    def extract_completion_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Extract course and module completion data.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (course_completions, module_completions, certificates) DataFrames\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"EXTRACTING COMPLETION DATA\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Course completions\n",
        "        course_completion_query = \"\"\"\n",
        "        SELECT\n",
        "            cc.id as completion_id,\n",
        "            cc.userid as user_id,\n",
        "            cc.course as course_id,\n",
        "            cc.timeenrolled as enrollment_timestamp,\n",
        "            cc.timestarted as started_timestamp,\n",
        "            cc.timecompleted as completed_timestamp,\n",
        "            cc.reaggregate,\n",
        "            FROM_UNIXTIME(cc.timeenrolled) as enrollment_date,\n",
        "            FROM_UNIXTIME(cc.timestarted) as started_date,\n",
        "            FROM_UNIXTIME(cc.timecompleted) as completed_date\n",
        "        FROM mdl_course_completions cc\n",
        "        WHERE cc.timecompleted IS NOT NULL\n",
        "        ORDER BY cc.userid, cc.course\n",
        "        \"\"\"\n",
        "        course_completions_df = fetch_data(self.engine, course_completion_query)\n",
        "\n",
        "        # Module completions\n",
        "        module_completion_query = \"\"\"\n",
        "        SELECT\n",
        "            cmc.id as module_completion_id,\n",
        "            cmc.coursemoduleid as course_module_id,\n",
        "            cmc.userid as user_id,\n",
        "            cmc.completionstate as completion_state,\n",
        "            cmc.timemodified as completion_timestamp,\n",
        "            FROM_UNIXTIME(cmc.timemodified) as completion_date\n",
        "        FROM mdl_course_modules_completion cmc\n",
        "        WHERE cmc.completionstate > 0\n",
        "        ORDER BY cmc.userid, cmc.coursemoduleid\n",
        "        \"\"\"\n",
        "        module_completions_df = fetch_data(self.engine, module_completion_query)\n",
        "\n",
        "        # Map completion states\n",
        "        module_completions_df['completion_state_label'] = module_completions_df['completion_state'].map({\n",
        "            1: 'Complete',\n",
        "            2: 'Complete (Pass)',\n",
        "            3: 'Complete (Fail)'\n",
        "        })\n",
        "\n",
        "        # Certificates issued\n",
        "        certificates_query = \"\"\"\n",
        "        SELECT\n",
        "            ci.id as certificate_issue_id,\n",
        "            ci.userid as user_id,\n",
        "            ci.customcertid as certificate_id,\n",
        "            ci.code as certificate_code,\n",
        "            ci.timecreated as issued_timestamp,\n",
        "            FROM_UNIXTIME(ci.timecreated) as issued_date\n",
        "        FROM mdl_customcert_issues ci\n",
        "        ORDER BY ci.userid, ci.customcertid\n",
        "        \"\"\"\n",
        "        certificates_df = fetch_data(self.engine, certificates_query)\n",
        "\n",
        "        print(f\"✓ Extracted {len(course_completions_df)} course completions\")\n",
        "        print(f\"✓ Extracted {len(module_completions_df)} module completions\")\n",
        "        print(f\"✓ Extracted {len(certificates_df)} certificates issued\")\n",
        "\n",
        "        self.extracted_data['course_completions'] = course_completions_df\n",
        "        self.extracted_data['module_completions'] = module_completions_df\n",
        "        self.extracted_data['certificates'] = certificates_df\n",
        "\n",
        "        return course_completions_df, module_completions_df, certificates_df\n",
        "\n",
        "    def extract_activity_logs(self, limit: int = 100000) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Extract user activity logs for engagement analysis.\n",
        "\n",
        "        Args:\n",
        "            limit: Maximum number of log entries to retrieve\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with activity log data\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"EXTRACTING ACTIVITY LOGS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        query = f\"\"\"\n",
        "        SELECT\n",
        "            l.id as log_id,\n",
        "            l.userid as user_id,\n",
        "            l.courseid as course_id,\n",
        "            l.eventname,\n",
        "            l.component,\n",
        "            l.action,\n",
        "            l.target,\n",
        "            l.timecreated as event_timestamp,\n",
        "            FROM_UNIXTIME(l.timecreated) as event_date,\n",
        "            l.origin,\n",
        "            l.ip\n",
        "        FROM mdl_logstore_standard_log l\n",
        "        WHERE l.userid > 2  -- Exclude system users\n",
        "        ORDER BY l.timecreated DESC\n",
        "        LIMIT {limit}\n",
        "        \"\"\"\n",
        "\n",
        "        logs_df = fetch_data(self.engine, query)\n",
        "\n",
        "        print(f\"✓ Extracted {len(logs_df)} activity log entries\")\n",
        "\n",
        "        self.extracted_data['activity_logs'] = logs_df\n",
        "        return logs_df\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_first_span_content(html_string):\n",
        "        \"\"\"\n",
        "        Extract meaningful visible text from HTML string.\n",
        "        - If it starts with a <span>, returns the first span’s text\n",
        "        - Otherwise returns all visible text joined cleanly\n",
        "        \"\"\"\n",
        "        if pd.isna(html_string):\n",
        "            return None\n",
        "        if not isinstance(html_string, str):\n",
        "            return html_string\n",
        "\n",
        "        soup = BeautifulSoup(html_string, \"html.parser\")\n",
        "\n",
        "        # If HTML has one or more <span>, return first span text\n",
        "        first_span = soup.find(\"span\")\n",
        "        if first_span and first_span.text.strip():\n",
        "            return first_span.text.strip()\n",
        "\n",
        "        # Otherwise collect all visible text\n",
        "        texts = [t.strip() for t in soup.stripped_strings if t.strip()]\n",
        "        return \" \".join(texts) if texts else \"\"\n",
        "\n",
        "    def extract_all_data(self):\n",
        "        \"\"\"Execute all extraction functions\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"STARTING COMPREHENSIVE DATA EXTRACTION\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        self.extract_users()\n",
        "        self.extract_courses()\n",
        "        self.extract_user_roles()\n",
        "        self.extract_enrollments()\n",
        "        self.extract_quiz_data()\n",
        "        self.extract_assignment_data()\n",
        "        self.extract_feedback_data()\n",
        "        self.extract_completion_data()\n",
        "        self.extract_activity_logs()\n",
        "\n",
        "        # Clean the columns\n",
        "        for name, df in self.extracted_data.items():\n",
        "            self.extracted_data[name] = df.applymap(self.extract_first_span_content)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"DATA EXTRACTION COMPLETE\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"\\nTotal datasets extracted: {len(self.extracted_data)}\")\n",
        "        print(\"\\nDataset summary:\")\n",
        "        for name, df in self.extracted_data.items():\n",
        "            print(f\"  - {name}: {len(df)} records\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 5: GOOGLE SHEETS EXPORT\n",
        "# ============================================================================\n",
        "\n",
        "def export_to_google_sheets(\n",
        "    extracted_data: Dict[str, pd.DataFrame],\n",
        "    spreadsheet_name: str = \"SUSTAIN MEL Data Export (RAW)\",\n",
        "    folder_name: str = \"SUSTAIN_ACADEMY\",\n",
        "):\n",
        "    import time\n",
        "    from google.colab import auth\n",
        "    import gspread\n",
        "    from gspread_dataframe import set_with_dataframe\n",
        "    from google.auth import default\n",
        "    from googleapiclient.discovery import build\n",
        "\n",
        "    auth.authenticate_user()\n",
        "    creds, _ = default(scopes=[\n",
        "        \"https://www.googleapis.com/auth/drive\",\n",
        "        \"https://www.googleapis.com/auth/spreadsheets\"\n",
        "    ])\n",
        "    gc = gspread.authorize(creds)\n",
        "    drive = build(\"drive\", \"v3\", credentials=creds)\n",
        "\n",
        "    # find or create folder\n",
        "    q = (\n",
        "        \"mimeType='application/vnd.google-apps.folder' \"\n",
        "        f\"and name='{folder_name}' and trashed=false\"\n",
        "    )\n",
        "    res = drive.files().list(q=q, fields=\"files(id,name)\").execute()\n",
        "    folder = res.get(\"files\", [])\n",
        "    if folder:\n",
        "        folder_id = folder[0][\"id\"]\n",
        "    else:\n",
        "        fld = {\"name\": folder_name, \"mimeType\": \"application/vnd.google-apps.folder\"}\n",
        "        folder_id = drive.files().create(body=fld, fields=\"id\").execute()[\"id\"]\n",
        "\n",
        "    # find existing spreadsheet\n",
        "    q = (\n",
        "        \"mimeType='application/vnd.google-apps.spreadsheet' \"\n",
        "        f\"and name='{spreadsheet_name}' and '{folder_id}' in parents and trashed=false\"\n",
        "    )\n",
        "    res = drive.files().list(q=q, fields=\"files(id,name)\").execute()\n",
        "    files = res.get(\"files\", [])\n",
        "\n",
        "    if files:\n",
        "        ss_id = files[0][\"id\"]\n",
        "        spreadsheet = gc.open_by_key(ss_id)\n",
        "        print(f\"Opening existing: {spreadsheet.url}\")\n",
        "    else:\n",
        "        spreadsheet = gc.create(spreadsheet_name)\n",
        "        file_id = spreadsheet.id\n",
        "        drive.files().update(\n",
        "            fileId=file_id,\n",
        "            addParents=folder_id,\n",
        "            removeParents=\"root\",\n",
        "            fields=\"id, parents\"\n",
        "        ).execute()\n",
        "        print(f\"Created new: {spreadsheet.url}\")\n",
        "\n",
        "    # write or overwrite sheets\n",
        "    for sheet_name, df in extracted_data.items():\n",
        "        if df.empty:\n",
        "            continue\n",
        "\n",
        "        title = sheet_name[:100]\n",
        "        try:\n",
        "            try:\n",
        "                ws = spreadsheet.worksheet(title)\n",
        "                spreadsheet.del_worksheet(ws)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            ws = spreadsheet.add_worksheet(\n",
        "                title=title,\n",
        "                rows=max(len(df) + 1, 2),\n",
        "                cols=max(len(df.columns), 1),\n",
        "            )\n",
        "            set_with_dataframe(ws, df)\n",
        "            time.sleep(1)\n",
        "            print(f\"Exported {title}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error {title}: {e}\")\n",
        "\n",
        "    return spreadsheet.url\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 7: MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    # Step 1: Get database password\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        db_password = userdata.get('db_password')\n",
        "        print(\"✓ Retrieved database password from Colab secrets\")\n",
        "    except:\n",
        "        # For local testing, use environment variable\n",
        "        import os\n",
        "        db_password = os.getenv('DB_PASSWORD')\n",
        "        if not db_password:\n",
        "            print(\"✗ Database password not found!\")\n",
        "            print(\"  Set 'db_password' in Colab secrets or DB_PASSWORD environment variable\")\n",
        "            return\n",
        "\n",
        "    # Step 2: Create database engine\n",
        "    engine = create_db_engine(db_password)\n",
        "\n",
        "    # Step 3: Extract all data\n",
        "    extractor = MELDataExtractor(engine)\n",
        "    extractor.extract_all_data()\n",
        "\n",
        "    # Step 4: Calculate MEL indicators\n",
        "    calculator = MELIndicatorCalculator(extractor.extracted_data)\n",
        "\n",
        "    # Step 6: Export to Google Sheets\n",
        "    all_data = {**extractor.extracted_data}\n",
        "    sheets_url = export_to_google_sheets(all_data)\n",
        "\n",
        "    # Step 7: Print summary\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EXECUTION COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\"\"\n",
        "Summary:\n",
        "  - Total datasets extracted: {len(extractor.extracted_data)}\n",
        "  - Documentation generated: MEL_Data_Documentation.md\n",
        "  - Google Sheets URL: {sheets_url if sheets_url else 'Export failed'}\n",
        "\n",
        "Next Steps:\n",
        "  1. Review the Google Sheets export\n",
        "  2. Read the documentation file\n",
        "  3. Set up automated dashboards\n",
        "  4. Configure scheduled reporting\n",
        "\n",
        "For support, refer to the documentation or contact the data team.\n",
        "\"\"\")\n",
        "\n",
        "    return {\n",
        "        'extracted_data': extractor.extracted_data,\n",
        "        'indicators': calculator.indicators,\n",
        "        'sheets_url': sheets_url\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# EXECUTE\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lGSi0EEj-rv7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOF2Htmus1Ep0mjSfJd5yW/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}